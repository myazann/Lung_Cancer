{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preparation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1LUvt8Y0wZET4jLHjEnS_CPQHgtWrqnuf",
      "authorship_tag": "ABX9TyNQe7XKvkx8nLe1jHdO92dD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myazann/Lung_Cancer/blob/main/Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrWhUm_56KSs"
      },
      "source": [
        "!git init\n",
        "!git pull https://github.com/myazann/TCIA-API-SDK.git\n",
        "\n",
        "!cp -r /content/tcia-rest-client-python/src/* /content\n",
        "\n",
        "!rm -rf tcia-rest-client-java/\n",
        "!rm -rf tcia-rest-client-python/\n",
        "\n",
        "!cp /content/drive/MyDrive/Annotation.zip /content\n",
        "!unzip Annotation.zip > /dev/null\n",
        "!rm Annotation.zip\n",
        "\n",
        "!pip install xmltodict\n",
        "!rm -rf sample_data\n",
        "!rm sample.py\n",
        "!rm README.md\n",
        "\n",
        "!pip install pip install pandasql\n",
        "from tciaclient import TCIAClient\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from xml.etree import cElementTree as ElementTree\n",
        "import xmltodict\n",
        "import urllib.request, urllib.error, urllib.parse\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import pandasql as ps\n",
        "\n",
        "tc = TCIAClient(baseUrl=\"https://services.cancerimagingarchive.net/services/v4\", resource = \"TCIA\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qktuafs7Ugb"
      },
      "source": [
        "##Get Annotations, Patients, Series ID's and SOP ID's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCkrh_fq-SHh"
      },
      "source": [
        "## Get bounding boxs associated with the images\n",
        "folders = np.array(os.listdir(\"Annotation\"))\n",
        "\n",
        "bboxs = {}\n",
        "\n",
        "for folder in folders:\n",
        "  path = \"Annotation/\" + folder\n",
        "  for img in os.listdir(path):\n",
        "    file_path = path + \"/\" + img\n",
        "    try:\n",
        "      dt = xmltodict.parse(ElementTree.tostring(ElementTree.parse(file_path).getroot()))[\"annotation\"][\"object\"]\n",
        "      if isinstance(dt, list):\n",
        "        box_list = []\n",
        "        for box in dt:\n",
        "          box_list.append(box)\n",
        "      else:\n",
        "        dt = dt[\"bndbox\"]\n",
        "      img_name = folder + \"-\" + img.split(\".x\")[0]\n",
        "      bboxs[img_name] = dt\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "with open('bboxs.json', \"w\") as json_file:\n",
        "  json.dump(bboxs, json_file)\n",
        "\n",
        "\n",
        "## Get the series id of each patient with the most images\n",
        "clctn = \"Lung-PET-CT-Dx\"\n",
        "series = json.loads(tc.get_series(collection = clctn, modality = \"CT\").read())\n",
        "ptnt_to_series_ids = pd.DataFrame()\n",
        "\n",
        "pt_id = []\n",
        "series_id = []\n",
        "img_count = []\n",
        "\n",
        "for elem in series:\n",
        "  try:\n",
        "    if elem[\"BodyPartExamined\"] == \"CHEST\":\n",
        "      pt_id.append(elem[\"PatientID\"].split(\"-\")[1])\n",
        "      series_id.append(elem[\"SeriesInstanceUID\"])\n",
        "      img_count.append(elem[\"ImageCount\"])\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "ptnt_to_series_ids[\"Patient_ID\"] = pt_id\n",
        "ptnt_to_series_ids[\"Series_ID\"] = series_id\n",
        "ptnt_to_series_ids[\"Image_Count\"] = img_count\n",
        "\n",
        "## Pandas groupby don't work this way so I used an SQL statement\n",
        "ptnt_to_series_ids = ps.sqldf(\"\"\"SELECT Patient_ID,MAX(Image_Count) Image_Count ,\n",
        "MAX(Series_ID) Series_ID FROM ptnt_to_series_ids\n",
        "GROUP BY Patient_ID\"\"\")\n",
        "\n",
        "ptnt_to_series_ids = dict((p,s) for p,s in ptnt_to_series_ids[[\"Patient_ID\",\"Series_ID\"]].values)\n",
        "\n",
        "## Get the image id's (SOP) in each series\n",
        "sop_to_series_ids = {}\n",
        "\n",
        "for series in ptnt_to_series_ids.values():\n",
        "  sop = json.loads(tc.get_SOP_instance(series).read())\n",
        "  for id in sop:\n",
        "    sop_to_series_ids[id[\"SOPInstanceUID\"]] = series\n",
        "\n",
        "imgs_with_bboxs = [box.split(\"-\")[1] for box in bboxs.keys()]\n",
        "sop_to_series_ids = dict((img, sop_to_series_ids[img]) for img in sop_to_series_ids.keys() if img in imgs_with_bboxs)\n",
        "\n",
        "with open('ptnt_to_series_ids.json', 'w') as json_file:\n",
        "  json.dump(ptnt_to_series_ids, json_file)\n",
        "\n",
        "with open('sop_to_series_ids.json', 'w') as json_file:\n",
        "  json.dump(sop_to_series_ids, json_file)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weSCHcGc7uSq"
      },
      "source": [
        "## Download images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_IeR-qPOxZj"
      },
      "source": [
        "if not os.path.exists(\"images\"):\n",
        "  os.mkdir(\"images\")\n",
        "  if os.path.exists(\"./drive/MyDrive/images.zip\"):\n",
        "    subprocess.call([\"cp\",\"./drive/MyDrive/images.zip\",\".\"])\n",
        "    subprocess.call([\"unzip\",\"images.zip\",\"-d\",\"images\"])\n",
        "\n",
        "existing_images = [img.split(\".d\")[0] for img in os.listdir(\"images\")]\n",
        "sop_to_series_ids = dict((img, sop_to_series_ids[img]) for img in sop_to_series_ids.keys() if img not in existing_images)\n",
        "\n",
        "for img in sop_to_series_ids.keys():\n",
        "  download_path = \"./images/\" + img + \".dcm\"\n",
        "  tc.get_single_image(sop_to_series_ids[img], img, downloadPath = download_path)\n",
        "  if len(os.listdir(\"images\")) % 1000 == 0:\n",
        "    print(\"Saving images!\")\n",
        "    subprocess.call([\"zip\",\"-r\",\"images.zip\",\"images\"])\n",
        "    subprocess.call([\"mv\",\"images.zip\",\"./drive/MyDrive/\"])\n",
        "\n",
        "\n",
        "subprocess.call([\"zip\",\"-r\",\"images.zip\",\"images\",\">\", \"/dev/null\"])\n",
        "subprocess.call([\"mv\",\"images.zip\",\"./drive/MyDrive/\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}